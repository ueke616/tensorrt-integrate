# 知识点
1. 对于整个tensorRT都进行了包装，直接引用自https://github.com/shouxieai/tensorRT_Pro
2. 接口友好程度相当高


案例18相比于案例17, 解决了YOLO封装的更多问题, 比如说解决了tensor 队列的堆积问题(满了就等待), 解决复用性问题 (相当于线程池的功能, 会预先分配一些)
总的来说就是更加内存安全了, 效率更高了

```cpp
// 向 tensor_allocator_ 申请一个tensor
// 目的是要用 GPU 做图像预处理
// 预处理完得到一坨GPU数据, 需要往队列里面抛
// jobs queue = [tensor1, tensor2, tensor3, ...]
// 造成队列堆积大量的 tensor
// 1. tensor 的复用性差, 因为你在 preprocess 上分配新的 tensor, 在 worker 中使用完又会释放 tensor。性能很差
// 2. 由于可能的堆积 (就是生产频率高, 消费频率底. commit 频率很高, infer 频率底, 很容易造成堆积)
//      堆积的结果就是显存占用很高, 导致系统不稳定, 无法长期运行
// 解决复用性问题
// 1. 使用一个 tensor_allocator_ 来管理tensor, 所有需要使用tensor的人, 找tensor_allocator_ 申请
//      预先会分配固定数量的 tensor, 比如说 10 个
//      如果申请的时候, 有空闲的 tensor 没有被分配出去, 则把这个空闲给他
//      如果申请的时候, 没有空闲的tensor, 此时, 让他等待
//      如果使用者使用完毕了, 他应该通知tensor_allocator_, 告诉他这个 tensor 他不用了, 你可以分配给别人了。
// 实现了 tensor 的复用, 他实现了申请数量太多, 处理不过来时让他等待的问题
// 其实也等于处理了队列上限问题
```
